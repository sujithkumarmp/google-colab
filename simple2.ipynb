{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNiUSsZGSDrhGzPcRfo+pd3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujithkumarmp/google-colab/blob/main/simple2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7AlySoIyk3q",
        "outputId": "0ed58b11-16bb-4234-9427-a90707fe7fa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 0, Cost: 0.6930893302140476\n",
            "Iteration 100, Cost: 0.6927596290405281\n",
            "Iteration 200, Cost: 0.6927511051647671\n",
            "Iteration 300, Cost: 0.6927425386865389\n",
            "Iteration 400, Cost: 0.6927339880750971\n",
            "Iteration 500, Cost: 0.6927254765593427\n",
            "Iteration 600, Cost: 0.6927170271613659\n",
            "Iteration 700, Cost: 0.6927086626027231\n",
            "Iteration 800, Cost: 0.6927004051680028\n",
            "Iteration 900, Cost: 0.6926922765737722\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "# Activation functions\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def sigmoid_derivative(z):\n",
        "    return sigmoid(z) * (1 - sigmoid(z))\n",
        "\n",
        "# Initialize parameters\n",
        "def initialize_parameters(layer_dims):\n",
        "    np.random.seed(42)\n",
        "    parameters = {}\n",
        "    for l in range(1, len(layer_dims)):\n",
        "        parameters[f\"W{l}\"] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
        "        parameters[f\"b{l}\"] = np.zeros((layer_dims[l], 1))\n",
        "    return parameters\n",
        "\n",
        "# Forward propagation\n",
        "def forward_propagation(X, parameters):\n",
        "    caches = {}\n",
        "    A = X\n",
        "    L = len(parameters) // 2  # Number of layers\n",
        "    for l in range(1, L + 1):\n",
        "        Z = np.dot(parameters[f\"W{l}\"], A) + parameters[f\"b{l}\"]\n",
        "        A = sigmoid(Z)\n",
        "        caches[f\"Z{l}\"] = Z\n",
        "        caches[f\"A{l-1}\"] = A if l > 1 else X\n",
        "    return A, caches\n",
        "\n",
        "# Compute cost\n",
        "def compute_cost(Y_hat, Y):\n",
        "    m = Y.shape[1]\n",
        "    cost = -np.sum(Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)) / m\n",
        "    return np.squeeze(cost)\n",
        "\n",
        "# Backward propagation\n",
        "def backward_propagation(Y_hat, Y, caches, parameters):\n",
        "    grads = {}\n",
        "    L = len(parameters) // 2\n",
        "    m = Y.shape[1]\n",
        "    dA = -(np.divide(Y, Y_hat) - np.divide(1 - Y, 1 - Y_hat))\n",
        "\n",
        "    for l in reversed(range(1, L + 1)):\n",
        "        dZ = dA * sigmoid_derivative(caches[f\"Z{l}\"])\n",
        "        grads[f\"dW{l}\"] = np.dot(dZ, caches[f\"A{l-1}\"].T) / m\n",
        "        grads[f\"db{l}\"] = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "        dA = np.dot(parameters[f\"W{l}\"].T, dZ)\n",
        "    return grads\n",
        "\n",
        "# Update parameters\n",
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    L = len(parameters) // 2\n",
        "    for l in range(1, L + 1):\n",
        "        parameters[f\"W{l}\"] -= learning_rate * grads[f\"dW{l}\"]\n",
        "        parameters[f\"b{l}\"] -= learning_rate * grads[f\"db{l}\"]\n",
        "    return parameters\n",
        "\n",
        "# Neural network model\n",
        "def neural_network(X, Y, layer_dims, learning_rate=0.01, iterations=1000):\n",
        "    parameters = initialize_parameters(layer_dims)\n",
        "    for i in range(iterations):\n",
        "        Y_hat, caches = forward_propagation(X, parameters)\n",
        "        cost = compute_cost(Y_hat, Y)\n",
        "        grads = backward_propagation(Y_hat, Y, caches, parameters)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        if i % 100 == 0:\n",
        "            print(f\"Iteration {i}, Cost: {cost}\")\n",
        "    return parameters\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Input data (X) and labels (Y)\n",
        "    X = np.random.rand(2, 500)  # 2 features, 500 examples\n",
        "    Y = (np.sum(X, axis=0) > 1).reshape(1, 500)  # Binary labels\n",
        "\n",
        "    # Define layer dimensions: [input, hidden, output]\n",
        "    layer_dims = [2, 4, 1]\n",
        "\n",
        "    # Train the neural network\n",
        "    trained_parameters = neural_network(X, Y, layer_dims, learning_rate=0.1, iterations=1000)"
      ]
    }
  ]
}