{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCUA9nlS1Kpdw7MUletWDP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujithkumarmp/google-colab/blob/main/gen-ai/cbow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"the dog barks at the fox\",\n",
        "    \"the fox is quick and the dog is lazy\"\n",
        "]\n",
        "\n",
        "# Preprocessing: Tokenize and build vocabulary\n",
        "def preprocess(corpus):\n",
        "    tokenized_corpus = [sentence.lower().split() for sentence in corpus]\n",
        "    vocab = set(word for sentence in tokenized_corpus for word in sentence)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    return tokenized_corpus, word_to_idx, idx_to_word\n",
        "\n",
        "tokenized_corpus, word_to_idx, idx_to_word = preprocess(corpus)\n",
        "\n",
        "# Generate CBOW training data\n",
        "def generate_cbow_data(tokenized_corpus, window_size=2):\n",
        "    data = []\n",
        "    for sentence in tokenized_corpus:\n",
        "        for idx, word in enumerate(sentence):\n",
        "            context = []\n",
        "            for w in range(-window_size, window_size + 1):\n",
        "                if w != 0 and 0 <= idx + w < len(sentence):\n",
        "                    context.append(sentence[idx + w])\n",
        "            target = word\n",
        "            data.append((context, target))\n",
        "    return data\n",
        "\n",
        "cbow_data = generate_cbow_data(tokenized_corpus)\n",
        "\n",
        "# Convert data to indices\n",
        "def data_to_indices(cbow_data, word_to_idx):\n",
        "    indexed_data = []\n",
        "    for context, target in cbow_data:\n",
        "        context_indices = [word_to_idx[word] for word in context]\n",
        "        target_index = word_to_idx[target]\n",
        "        indexed_data.append((context_indices, target_index))\n",
        "    return indexed_data\n",
        "\n",
        "indexed_data = data_to_indices(cbow_data, word_to_idx)\n",
        "\n",
        "# CBOW Model\n",
        "class CBOWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(CBOWModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.linear = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context):\n",
        "        embedded = self.embeddings(context).mean(dim=0)\n",
        "        out = self.linear(embedded)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 10\n",
        "vocab_size = len(word_to_idx)\n",
        "model = CBOWModel(vocab_size, embedding_dim)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "# Training\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for context_indices, target_index in indexed_data:\n",
        "        context_tensor = torch.tensor(context_indices, dtype=torch.long)\n",
        "        target_tensor = torch.tensor([target_index], dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(context_tensor)\n",
        "        loss = criterion(output.unsqueeze(0), target_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Testing embeddings\n",
        "word_embeddings = model.embeddings.weight.data\n",
        "for word, idx in word_to_idx.items():\n",
        "    print(f\"Word: {word}, Embedding: {word_embeddings[idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ4Y6e7Sjrgk",
        "outputId": "faa4078e-7083-40a5-a5d3-7d97d3cda1dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 55.0607\n",
            "Epoch 20, Loss: 51.2209\n",
            "Epoch 30, Loss: 48.4199\n",
            "Epoch 40, Loss: 45.9719\n",
            "Epoch 50, Loss: 43.7157\n",
            "Epoch 60, Loss: 41.5963\n",
            "Epoch 70, Loss: 39.5877\n",
            "Epoch 80, Loss: 37.6735\n",
            "Epoch 90, Loss: 35.8398\n",
            "Epoch 100, Loss: 34.0738\n",
            "Word: barks, Embedding: tensor([ 1.1789,  0.5315,  0.5461,  0.3272, -0.0911, -0.0412,  1.0795, -0.1773,\n",
            "         0.3417, -0.0418])\n",
            "Word: brown, Embedding: tensor([-0.7878,  1.9682, -0.8545,  0.6659,  0.5404,  0.2591, -1.5156,  0.3505,\n",
            "         0.6164,  0.5290])\n",
            "Word: lazy, Embedding: tensor([ 0.8970, -1.6502, -2.2585,  0.7454,  0.9933,  0.0064, -1.0858, -0.3764,\n",
            "        -0.7918,  1.1950])\n",
            "Word: over, Embedding: tensor([-0.7783,  1.5325,  0.8425, -0.7505,  0.3283, -0.1816, -0.6441,  1.0973,\n",
            "         0.2709, -0.0450])\n",
            "Word: is, Embedding: tensor([ 1.0826, -0.1952,  0.3646, -0.7235, -0.8089, -0.5043, -0.1451,  1.0848,\n",
            "         0.2767, -0.7324])\n",
            "Word: jumps, Embedding: tensor([ 0.1541,  1.1651,  0.5360,  1.4104, -2.5237, -2.3134, -0.0139,  0.3956,\n",
            "         0.9314, -0.3080])\n",
            "Word: at, Embedding: tensor([ 2.7617, -0.4223,  0.8762, -0.5758, -1.0701,  1.0162, -0.1508, -0.1208,\n",
            "        -0.1859,  1.1596])\n",
            "Word: fox, Embedding: tensor([-0.2154,  1.9898, -0.4576, -0.4412, -0.1642,  1.0966, -0.0173, -1.7169,\n",
            "         1.7062,  0.0958])\n",
            "Word: dog, Embedding: tensor([-0.8925,  1.1184,  0.3440, -1.2621,  0.4697, -0.3662, -0.1682, -0.3743,\n",
            "        -1.9506,  1.3906])\n",
            "Word: and, Embedding: tensor([-0.4433, -0.8413, -1.2339,  1.3714,  0.4344,  1.3593,  1.0341,  0.9698,\n",
            "         0.0775,  1.2280])\n",
            "Word: quick, Embedding: tensor([ 1.4157,  0.4220,  0.9304, -0.8228, -2.1028, -0.1476, -1.0331,  0.2133,\n",
            "        -1.5798,  0.1099])\n",
            "Word: the, Embedding: tensor([-0.3589,  2.3559, -0.3915, -0.7983,  0.0610, -0.0137, -0.2967,  0.3897,\n",
            "         0.1671,  1.3426])\n"
          ]
        }
      ]
    }
  ]
}