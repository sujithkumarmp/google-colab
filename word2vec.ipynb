{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNKFRE9baUQ43cBGknlEZi2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sujithkumarmp/google-colab/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "# Sample corpus\n",
        "corpus = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "    \"the dog barked at the fox\",\n",
        "    \"the fox ran away quickly\"\n",
        "]\n",
        "\n",
        "# Preprocessing: Tokenize and build vocabulary\n",
        "tokenized_corpus = [sentence.split() for sentence in corpus]\n",
        "vocab = list(set(chain(*tokenized_corpus)))\n",
        "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "\n",
        "# Generate n-grams (context, target pairs)\n",
        "def generate_skipgram_data(tokenized_corpus, window_size=2):\n",
        "    data = []\n",
        "    for sentence in tokenized_corpus:\n",
        "        for idx, word in enumerate(sentence):\n",
        "            for neighbor in range(-window_size, window_size + 1):\n",
        "                if neighbor != 0 and 0 <= idx + neighbor < len(sentence):\n",
        "                    data.append((word, sentence[idx + neighbor]))\n",
        "    return data\n",
        "\n",
        "data = generate_skipgram_data(tokenized_corpus)\n",
        "\n",
        "# Convert data to indices\n",
        "data_idx = [(word_to_idx[target], word_to_idx[context]) for target, context in data]\n",
        "\n",
        "# Model definition\n",
        "class Word2Vec(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.output_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, target):\n",
        "        embed = self.embeddings(target)\n",
        "        out = self.output_layer(embed)\n",
        "        return out\n",
        "\n",
        "# Hyperparameters\n",
        "embedding_dim = 10\n",
        "learning_rate = 0.01\n",
        "epochs = 100\n",
        "\n",
        "# Initialize model, loss, and optimizer\n",
        "model = Word2Vec(len(vocab), embedding_dim)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for target, context in data_idx:\n",
        "        target_tensor = torch.tensor([target], dtype=torch.long)\n",
        "        context_tensor = torch.tensor([context], dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(target_tensor)\n",
        "        loss = criterion(output, context_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch {epoch + 1}, Loss: {total_loss:.4f}\")\n",
        "\n",
        "# Extract embeddings\n",
        "embeddings = model.embeddings.weight.data\n",
        "print(\"Word Embeddings:\")\n",
        "for word, idx in word_to_idx.items():\n",
        "    print(f\"{word}: {embeddings[idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQ4Y6e7Sjrgk",
        "outputId": "b1d0daf7-323a-425c-ca62-d2209dce4379"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10, Loss: 141.2042\n",
            "Epoch 20, Loss: 133.9104\n",
            "Epoch 30, Loss: 129.3713\n",
            "Epoch 40, Loss: 125.7105\n",
            "Epoch 50, Loss: 122.4689\n",
            "Epoch 60, Loss: 119.5148\n",
            "Epoch 70, Loss: 116.8338\n",
            "Epoch 80, Loss: 114.4493\n",
            "Epoch 90, Loss: 112.3806\n",
            "Epoch 100, Loss: 110.6280\n",
            "Word Embeddings:\n",
            "barked: tensor([-1.2124, -0.8199,  0.2841,  1.2619, -1.6186, -0.9090, -0.1290, -0.8578,\n",
            "        -0.2008, -0.9770])\n",
            "brown: tensor([ 0.6599, -0.3906, -1.4428,  0.0633, -0.8937,  0.6748, -0.6055,  1.1687,\n",
            "        -1.6470,  0.7958])\n",
            "lazy: tensor([ 1.3022,  0.1532, -0.7149, -0.0339, -2.4266, -0.9491, -0.9831,  2.0436,\n",
            "         1.6752,  1.8785])\n",
            "over: tensor([ 0.7557,  0.5107, -0.9613, -0.5901, -0.6587,  0.8117,  2.6235,  1.2232,\n",
            "        -0.4878,  1.9021])\n",
            "away: tensor([-0.5040,  0.0285, -1.3486, -0.8679, -0.2043,  0.8493, -0.0173, -0.1751,\n",
            "         0.1447, -0.4218])\n",
            "quickly: tensor([ 1.1940,  0.5059, -1.5792, -1.5513, -0.1756,  0.3794, -0.7068,  0.5551,\n",
            "        -0.3265,  1.3256])\n",
            "jumps: tensor([-0.4065,  0.2968,  2.2834, -0.6891, -0.5016, -2.4600,  0.7385,  0.6456,\n",
            "        -0.6413,  1.5717])\n",
            "ran: tensor([-1.7074, -0.6289,  0.3610, -0.1596, -1.2883,  1.2936, -0.0781, -1.0844,\n",
            "        -0.4153,  0.0557])\n",
            "at: tensor([ 0.4718, -1.4405, -1.2526, -1.2641,  1.8525, -0.3591,  1.3760,  0.5074,\n",
            "        -1.9397,  0.5052])\n",
            "fox: tensor([-1.6027,  0.9028,  0.4811,  1.8635,  0.9350,  0.9075, -0.9028,  0.6778,\n",
            "         0.6791,  0.1656])\n",
            "dog: tensor([-0.3265, -1.3404,  0.6275,  0.0908, -0.4097, -0.4607,  0.4198,  3.2771,\n",
            "         0.7638,  0.1017])\n",
            "quick: tensor([-0.6674, -0.6603,  0.8962, -0.4711, -2.5982,  1.2131, -0.8720,  0.1573,\n",
            "         0.3043,  0.7495])\n",
            "the: tensor([ 1.1891,  1.5818,  1.5286, -0.6751, -0.3180,  0.7170,  1.0233, -0.1395,\n",
            "         0.4485, -1.2245])\n"
          ]
        }
      ]
    }
  ]
}